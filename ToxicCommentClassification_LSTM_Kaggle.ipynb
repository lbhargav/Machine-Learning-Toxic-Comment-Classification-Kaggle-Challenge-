{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ToxicCommentClassification_LSTM_Kaggle.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "LNAZl9VtN7zs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Toxic comment classification -LSTM- kaggle challenge**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "C-29buciiOlI",
        "outputId": "b360b1c1-bc06-4de9-b465-9d06c87eff4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0uG0MBvMR-4r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Importing Libraries "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "M5yFMVgOvqri",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e363768a-51b6-4616-aca5-6f0b817a409f"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import io\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Embedding, Input\n",
        "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "w-H1gXlkSg1E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Loading data"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "f6VBnqNSjv-D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "1bdc5d15-59fa-4c83-b7b8-2fb4ebd73eeb"
      },
      "cell_type": "code",
      "source": [
        "train=pd.read_csv('drive/My Drive/Toxic_Comment/train.csv') \n",
        "test=pd.read_csv('drive/My Drive/Toxic_Comment/test.csv') \n",
        "embedding_file = 'drive/My Drive/Toxic_Comment/glove.6B.50d.txt'\n",
        "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "y = train[list_classes].values\n",
        "train.head(5)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id                                       comment_text  toxic  \\\n",
              "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
              "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
              "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
              "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
              "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
              "\n",
              "   severe_toxic  obscene  threat  insult  identity_hate  \n",
              "0             0        0       0       0              0  \n",
              "1             0        0       0       0              0  \n",
              "2             0        0       0       0              0  \n",
              "3             0        0       0       0              0  \n",
              "4             0        0       0       0              0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "wS02Xrv9S5L0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "ed583cf1-dd01-48e3-d586-d9ab23333d88"
      },
      "cell_type": "code",
      "source": [
        "train.apply(lambda x: sum(x.isnull()), axis=0)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id               0\n",
              "comment_text     0\n",
              "toxic            0\n",
              "severe_toxic     0\n",
              "obscene          0\n",
              "threat           0\n",
              "insult           0\n",
              "identity_hate    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "K_X01d0QqMbJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e717b238-9e2a-4680-9184-fa231a36d546"
      },
      "cell_type": "code",
      "source": [
        "print(len(train))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "116211\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HGNf4r7fThC3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "ca09f400-36c3-435d-8178-b1b27324732c"
      },
      "cell_type": "code",
      "source": [
        "print(\"class distribution of each class\")\n",
        "for i in list_classes:\n",
        "  print(train[i].value_counts())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "class distribution of each class\n",
            "0    105077\n",
            "1     11134\n",
            "Name: toxic, dtype: int64\n",
            "0    115055\n",
            "1      1156\n",
            "Name: severe_toxic, dtype: int64\n",
            "0    110048\n",
            "1      6163\n",
            "Name: obscene, dtype: int64\n",
            "0    115853\n",
            "1       358\n",
            "Name: threat, dtype: int64\n",
            "0    110471\n",
            "1      5740\n",
            "Name: insult, dtype: int64\n",
            "0    115220\n",
            "1       991\n",
            "Name: identity_hate, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Nhz7z8uAXBeu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Preparing embedding dictionary reading from embedding text file"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "oeb8ngZIrVoZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_embedding_coefs(word, *arr): \n",
        "  return word, np.asarray(arr, dtype='float32')\n",
        "embeddings_index = dict(get_embedding_coefs(*o.strip().split()) for o in open(embedding_file,'r', encoding=\"utf8\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qgG_7xQkU-Rp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "list_sentences_train = train[\"comment_text\"]\n",
        "list_sentences_test = test[\"comment_text\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s1iaOTExWRQL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Tokenizing the text comments"
      ]
    },
    {
      "metadata": {
        "id": "n6_ZMCYCQxH5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_features = 2000\n",
        "maxlen = 100\n",
        "\n",
        "tokenizer = text.Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(list(list_sentences_train))\n",
        "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
        "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
        "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
        "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uec2gIhDXGmQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Preparing embedding matrix"
      ]
    },
    {
      "metadata": {
        "id": "dU3uw0RjQxIG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_embs = np.stack(embeddings_index.values())\n",
        "emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
        "embed_size = 50\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sfNHGVqlWqni",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Model selection"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HkEhSlZDrKIm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_model():\n",
        "    embed_size = 128\n",
        "    inp = Input(shape=(maxlen, ))\n",
        "    x = Embedding(max_features, embed_size)(inp)\n",
        "    x = Bidirectional(LSTM(128, return_sequences=True,dropout=0.5,recurrent_dropout=0.1))(x)\n",
        "    x = GlobalMaxPool1D()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(50, activation=\"relu\")(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(6, activation=\"sigmoid\")(x)\n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "USeocgYdQxIe",
        "colab_type": "code",
        "outputId": "5379c28d-7438-40bd-ca43-d38aed915a21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "cell_type": "code",
      "source": [
        "model = get_model()\n",
        "batch_size = 32\n",
        "epochs = 2\n",
        "model.fit(X_train, y, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 104589 samples, validate on 11622 samples\n",
            "Epoch 1/2\n",
            "104589/104589 [==============================] - 1744s 17ms/step - loss: 0.0877 - acc: 0.9733 - val_loss: 0.0565 - val_acc: 0.9808\n",
            "Epoch 2/2\n",
            "104589/104589 [==============================] - 1750s 17ms/step - loss: 0.0636 - acc: 0.9793 - val_loss: 0.0558 - val_acc: 0.9808\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f91ea83b8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "9rFfuFXUQxI8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_test = model.predict(X_test)\n",
        "sample_submission = pd.read_csv('drive/My Drive/Toxic_comment/sample_submission.csv')\n",
        "sample_submission[list_classes] = y_test\n",
        "sample_submission.to_csv(\"baseline.csv\", index=False)\n",
        "files.download('baseline.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vnSE9wOvlWcC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import CuDNNLSTM, Dense, Bidirectional"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DciCyABdlc6V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_model():\n",
        "    embed_size = 300\n",
        "    inp = Input(shape=(maxlen, ))\n",
        "    x = Embedding(max_features, embed_size)(inp)\n",
        "    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
        "    x = Bidirectional(CuDNNLSTM(64))(x)\n",
        "    x = Dense(50, activation=\"relu\")(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(6, activation=\"sigmoid\")(x)\n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QxFTPCQHleKj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "738e4f73-0c22-4aca-922c-b2035d0bec39"
      },
      "cell_type": "code",
      "source": [
        "model = get_model()\n",
        "batch_size = 32\n",
        "epochs = 2\n",
        "model.fit(X_train, y, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 104589 samples, validate on 11622 samples\n",
            "Epoch 1/2\n",
            "104589/104589 [==============================] - 346s 3ms/step - loss: 0.0769 - acc: 0.9766 - val_loss: 0.0563 - val_acc: 0.9809\n",
            "Epoch 2/2\n",
            "104589/104589 [==============================] - 343s 3ms/step - loss: 0.0599 - acc: 0.9800 - val_loss: 0.0575 - val_acc: 0.9805\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f91c76b6da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "v86RY2gYlq2b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}